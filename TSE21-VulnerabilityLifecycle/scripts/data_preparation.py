import json
import pandas as pd
import datetime as dt

in_file_path = "../data/repos_enriched.json"
in_removal_filepath = "../data/removal_methods.csv"
rq2_discarded = "../data/rq2_discarded.csv"
rq3_discarded = "../data/rq3_discarded.csv"
rq1_input = "../data/rq1_input.csv"
rq2_input = "../data/rq2_input.csv"
rq3_input = "../data/rq3_input.csv"
rq4_input = "../data/rq4_input.csv"

newfeature_keywords = {'new', 'feature', 'add', 'creat', 'introduc', 'implement'}
bugfixing_keywords = {'fix', 'repair', 'error', 'bug', 'issue', 'exception'}
enhancement_keywords = {'updat', 'modif', 'upgrad', 'export', 'remov', 'integrat', 'support', 'enhancement', 'replac', 'includ', 'expos', 'generat', 'migrat'}
refactoring_keywords = {'renam', 'reorganiz', 'refactor', 'clean', 'polish', 'mov', 'extract', 'reorder', 're-order', 'merg'}

repos = []
with open(in_file_path, "r") as in_file:
    repos = json.load(in_file)

def check_message(row):
    lower_message = row["message"].lower()
    goals = {
        "new_feature": int(any(keyword in lower_message for keyword in newfeature_keywords)),
        "bug_fixing": int(any(keyword in lower_message for keyword in bugfixing_keywords)),
        "enhancement": int(any(keyword in lower_message for keyword in enhancement_keywords)),
        "refactoring": int(any(keyword in lower_message for keyword in refactoring_keywords))
    }
    return pd.Series(goals)


def compute_time_distances(row):
    date_format = "%Y-%m-%d %H:%M:%S%z"
    date = dt.datetime.strptime(row["date"], date_format)
    nearest_release_date = dt.datetime.strptime(row["nearest_release_date"], date_format)
    creation_date = dt.datetime.strptime(row["creation_date"], date_format)
    distance_from_release = nearest_release_date - date
    distance_from_creation = date - creation_date
    time_distances = {
        "distance_from_release": distance_from_release.days,
        "distance_from_creation": distance_from_creation.days
    }
    return pd.Series(time_distances)

def compute_days_distance(cve):
    date_format = "%Y-%m-%d %H:%M:%S%z"
    last_fix_date = dt.datetime.strptime(cve["last_fix_date"], date_format)
    turning_date = dt.datetime.strptime(cve["turning_date"], date_format)
    return (last_fix_date - turning_date).days

# RQ1
print("Preparing Input Dataset for RQ1")

data_rq1 = []
for repo in repos:
    for cve in repo["cves"]:
        for fix in cve["fixes"]:
            for inducing in fix["inducings"]:
                row = [
                    repo["repo"],
                    inducing["hash"],
                    inducing["date"],
                    cve["cve"],
                    cve["cwe"],
                    inducing["file"],
                    inducing["added_lines"],
                    inducing["removed_lines"],
                    inducing["commits_since_file_creation"]
                ]
                data_rq1.append(row)
cols = ["repo", "hash", "date", "cve", "cwe", "file", "added_lines", "removed_lines", "commits_since_file_creation"]
df_rq1 = pd.DataFrame(data_rq1, columns=cols)

## Drop duplicates
old_len = len(df_rq1)
df_rq1.drop_duplicates(ignore_index=True, inplace=True)
new_len = len(df_rq1)
print("Removed {} Duplicated Entries (caused by fixes that shared the very same inducing commits)".format(old_len - new_len))

df_rq1.to_csv(rq1_input, index=False)
print("RQ1 Done!")
print()

# RQ2
print("Preparing Input Dataset for RQ2")

data_rq2 = []
for repo in repos:
    for cve in repo["cves"]:
        for fix in cve["fixes"]:
            for inducing in fix["inducings"]:
                row = [
                    repo["repo"],
                    inducing["hash"],
                    repo["creation_date"],
                    inducing["message"],
                    inducing["author"],
                    inducing["author_workload"],
                    inducing["author_tenure"],
                    inducing["date"],
                    inducing["nearest_release_date"],
                    cve["cve"],
                    cve["cwe"],
                ]
                data_rq2.append(row)
cols = ["repo", "hash", "creation_date", "message", "author", "author_workload", "author_tenure", "date", "nearest_release_date", "cve", "cwe"]
df_rq2 = pd.DataFrame(data_rq2, columns=cols)

## Drop duplicates
old_len = len(df_rq2)
df_rq2.drop_duplicates(subset=["repo", "hash"], ignore_index=True, inplace=True)
new_len = len(df_rq2)
print("Removed {} Duplicated Entries (caused by fixes that shared the very same inducing commits)".format(old_len - new_len))

## Discard entries that have "" as nearest_release_date (sadly, we are forced to do so)
initial_num_inducings = df_rq2.groupby(["repo", "hash"]).size().count()
df_discarded_rq2 = df_rq2[df_rq2["nearest_release_date"] == ""]
df_rq2 = df_rq2[df_rq2["nearest_release_date"] != ""]

## Commit goals categorisation => for each inducing commit, check the message and add 4 new columns representing the commit goal 
commit_goals = df_rq2.apply(lambda row: check_message(row), axis=1)
df_rq2 = pd.concat([df_rq2, commit_goals], axis=1)

## Project status => for each inducing commit, compute the difference between date and [nearest_release_date/creation_date]
project_status = df_rq2.apply(lambda row: compute_time_distances(row), axis=1)
df_rq2 = pd.concat([df_rq2, project_status], axis=1)

## TODO Produce another dataset for discarded entries
## Discard entries that have negative distance_from_release (cases of rebase or amends)
intermediate_num_inducings = df_rq2.groupby(["repo", "hash"]).size().count()
df_discarded_rq2.append(df_rq2[df_rq2["distance_from_release"] < 0])
df_rq2 = df_rq2[df_rq2["distance_from_release"] >= 0]
num_inducings = df_rq2.groupby(["repo", "hash"]).size().count()
print("Discarded {} Inducing Commits (of which, {} have no Nearest Release Date and {} were amended or rebased)".format(initial_num_inducings - num_inducings, initial_num_inducings - intermediate_num_inducings, intermediate_num_inducings - num_inducings))

df_rq2.to_csv(rq2_input, index=False)
df_discarded_rq2.to_csv(rq2_discarded, index=False)
print("RQ2 Done!")
print()

# RQ3
print("Preparing Input Dataset for RQ3")

fixes = []
inducings = []
for repo in repos:
    for cve in repo["cves"]:
        for fix in cve["fixes"]:
            fixes.append([
                repo["repo"],
                cve["cve"],
                fix["hash"],
                fix["date"],
                len(fix["inducings"])
            ])
            for inducing in fix["inducings"]:
                row = [
                    repo["repo"],
                    cve["cve"],
                    cve["cwe"],
                    inducing["hash"],
                    inducing["date"],
                    fix["date"],
                    inducing["commits_before_fix"],
                    fix["commits_before_last_fix"]
                ]
                inducings.append(row)
fixes_cols = ["repo", "cve", "hash", "date", "num_inducings"]
inducing_cols = ["repo", "cve", "cwe", "hash", "inducing_date", "fix_date", "commits_before_fix", "commits_between_fix_and_last_fix"]
df_rq3_fixes = pd.DataFrame(fixes, columns=fixes_cols)
df_rq3_inducings = pd.DataFrame(inducings, columns=inducing_cols)

## Drop duplicates from both dataframes
old_len = len(df_rq3_inducings)
df_rq3_inducings.drop_duplicates(ignore_index=True, inplace=True)
new_len = len(df_rq3_inducings)
print("Removed {} Duplicated Entries from Inducing Commits".format(old_len - new_len))
old_len = len(df_rq3_fixes)
df_rq3_fixes.drop_duplicates(ignore_index=True, inplace=True)
new_len = len(df_rq3_fixes)
print("Removed {} Duplicated Entries from Fixing Commits".format(old_len - new_len))

## Do not consider fixes without inducing commits
df_rq3_fixes = df_rq3_fixes[df_rq3_fixes["num_inducings"] != 0]

## Group by CVE and get all CVE turning points and last fixes
inducings_cve_groups = df_rq3_inducings.groupby("cve", sort=False)
fixes_cve_groups = df_rq3_fixes.groupby("cve", sort=False)
df_turnings = df_rq3_inducings[inducings_cve_groups["inducing_date"].transform(max) == df_rq3_inducings["inducing_date"]]
df_last_fixes = df_rq3_fixes[fixes_cve_groups["date"].transform(max) == df_rq3_fixes["date"]]

## In cases where CVE has multiple fixes that shares the same turning point, we consider the first one
df_turnings = df_turnings.drop_duplicates(subset=["cve"], ignore_index=True)

## In cases where CVE has multiple fixes with same date, we consider the first one
df_last_fixes = df_last_fixes.drop_duplicates(subset=["cve"], ignore_index=True)

## Join the two dataframes on CVE column
df_rq3 = pd.merge(df_turnings, df_last_fixes, on="cve")
df_rq3.rename({
    "hash_x": "turning_hash",
    "hash_y": "last_fix_hash",
    "inducing_date": "turning_date",
    "date": "last_fix_date"
}, axis=1, inplace=True)

## Compute the time/commits differences
df_rq3["days_before_last_fix"] = df_rq3.apply(lambda row: compute_days_distance(row), axis=1)
df_rq3["commits_before_last_fix"] = df_rq3["commits_before_fix"] + df_rq3["commits_between_fix_and_last_fix"]
df_rq3 = df_rq3[["cve", "cwe", "turning_hash", "last_fix_hash", "days_before_last_fix", "commits_before_last_fix"]]

## Discard negative time cases
df_discarded_rq3 = df_rq3[df_rq3["days_before_last_fix"] < 0]
print("Discarded {} CVEs (due to rebases or amends)".format(len(df_discarded_rq3)))
df_rq3 = (df_rq3[df_rq3["days_before_last_fix"] >= 0]).reset_index(drop=True)

df_rq3.to_csv(rq3_input, index=False)
df_discarded_rq3.to_csv(rq3_discarded, index=False)
print("RQ3 Done!")
print()

# RQ4
print("Preparing Input Dataset for RQ4")

cves = []
for repo in repos:
    for cve in repo["cves"]:
        commits = 0
        files = set()
        added_lines = 0
        removed_lines = 0
        for fix in cve["fixes"]:
            commits += 1
            added_lines += fix["added_lines"]
            removed_lines += fix["removed_lines"]
            files.update(fix["files"])
        cves.append([
            cve["cve"],
            cve["cwe"],
            repo["repo"],
            commits,
            len(files),
            added_lines,
            removed_lines
        ])
cve_cols = ["cve", "cwe", "repo", "number_fixes", "number_files", "total_added_lines", "total_removed_lines"]
df_cves = pd.DataFrame(cves, columns=cve_cols)
df_removal = pd.read_csv(in_removal_filepath, delimiter=";")

## Drop duplicates from both dataframes
old_len = len(df_cves)
df_cves.drop_duplicates(ignore_index=True, inplace=True)
new_len = len(df_cves)
print("Removed {} Duplicated Entries from CVEs".format(old_len - new_len))
old_len = len(df_removal)
df_removal.drop_duplicates(ignore_index=True, inplace=True)
new_len = len(df_removal)
print("Removed {} Duplicated Entries from Removal Methods".format(old_len - new_len))

## Join the two dataframes on CVE column
df_rq4 = pd.merge(df_cves, df_removal, on="cve", how="right")
df_rq4.drop_duplicates(ignore_index=True, inplace=True)

df_rq4.drop(["description"], axis=1, inplace=True)
df_rq4.to_csv(rq4_input, index=False)
print("RQ4 Done!")
print()

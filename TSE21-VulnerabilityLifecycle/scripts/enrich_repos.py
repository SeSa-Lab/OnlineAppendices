import json
import re
import os
import shutil
import operator
import datetime as dt
from git import Repo, GitCommandError, BadName
from pydriller import GitRepository
from collections import Counter


in_filepath = "../data/cves.json"
out_filepath = "../data/repos_enriched.json"
tmp = "/tmp/"
invalid_extensions = ('.txt', '.md', '.man', '.lang', '.loc', '.tex', '.texi', '.rst',
'.gif', '.png', '.jpg', '.jpeg', '.svg', '.ico',
'.css', '.scss', '.less',
'.gradle', '.ini',
'.zip',
'.pdf')
exclusions = r"^(install|changelog(s)?|change(s)?|author(s)?|news|readme|todo|about(s)?|credit(s)?|license|release(s)?|release(s)?|release(_|-)note(s)?|version(s)?|makefile|pom|\.git.*|\.travis|\.classpath|\.project)$"


def get_repos(cves):
    repos = []
    for cve in cves:
        for fix in cve["fixes"]:
            repos.append(fix["repo"])
    # Remove duplicates but keep the ordering
    return list(dict.fromkeys(repos))


def clone_repo(repo_url):
    project_name = repo_url.rsplit('/', 1)[1]
    repo_filepath = tmp + project_name
    if not os.path.exists(repo_filepath):
        parts = repo_url.split('://', 1)
        # This automatically skips private repositories
        repo_url_no_credentials = parts[0] + "://:@" + parts[1]
        try:
            print("Cloning from remote...")
            Repo.clone_from(repo_url_no_credentials, repo_filepath)
            print("Cloning done!")
        except GitCommandError as ger:
            raise ger
    else:
        print("Repository already cloned")
    git_repo = GitRepository(repo_filepath)
    git_repo.reset()
    return git_repo


def get_authors_first_authored_date(commits):
    # Ensure they are sorted by author_date to be robust to amendments
    commits.sort(key=lambda c: c.author_date)
    authors_first_date = {}
    for commit in commits:
        author = commit.author.email
        if author not in authors_first_date.keys():
            authors_first_date[author] = commit.author_date
    return authors_first_date


def get_workloads(git_repo, commits, reference_date):
    first_day = reference_date.replace(day=1)
    next_month = reference_date.replace(day=28) + dt.timedelta(days=4)
    last_day = next_month - dt.timedelta(days=next_month.day)
    authors = []
    for commit in commits:
        author_date = commit.author_date
        if first_day <= author_date and author_date <= last_day:
            authors.append(commit.author.email)
    counter = Counter(authors)
    return counter


def get_tenures(git_repo, authors_first_date, reference_date):
    tenures = {}
    for author in authors_first_date.keys():
        first_date = authors_first_date[author]
        elapsed_months = (reference_date.year - first_date.year) * \
            12 + reference_date.month - first_date.month
        if elapsed_months >= 0:
            tenures[author] = elapsed_months
    return tenures


def get_percentile(distribution, key):
    min_value = min(distribution.values())
    max_value = max(distribution.values())
    if max_value == min_value:
        return 1.0
    else:
        percentile = float(distribution[key] - min_value) / float(max_value - min_value)
        return percentile


def is_invalid_extension(file_path):
    extension = os.path.splitext(file_path)[1]
    return extension in invalid_extensions


def is_invalid_file(file_path):
    name = os.path.splitext(os.path.basename(file_path))[0]
    return re.match(exclusions, name, re.IGNORECASE)


def is_test_file(file_path):
    dir = '/' + os.path.dirname(file_path) + '/'
    name = os.path.splitext(os.path.basename(file_path))[0]
    directory_match = re.match(r"^.*\/tests?\/.*$", dir, re.IGNORECASE)
    prefix_match = re.match(r"^test.+", name, re.IGNORECASE)
    postfix_match = re.match(r".+test$", name, re.IGNORECASE)
    return directory_match or prefix_match or postfix_match


def get_adjusted_message(message):
    message_no_carriage = message.replace("\r", "\n")
    one_newline_message = re.sub(r"\n+", "\n", message_no_carriage)
    clear_message = one_newline_message.replace("\n", ". ").replace(
        "\t", " ").replace(",", " ").replace("\"", "'")
    stripped_message = clear_message.strip()
    return re.sub(r" +", " ", stripped_message)


def get_commit_info(git_repo, commit_hash, fix_hash, modified_file, commits, authors_first_date):
    commit = git_repo.get_commit(commit_hash)
    message = get_adjusted_message(commit.msg)
    date = commit.author_date

    author = commit.author.email
    workloads = get_workloads(git_repo, commits, date)
    tenures = get_tenures(git_repo, authors_first_date, date)
    
    print("Getting info from {}".format(commit_hash))
    author_workload = get_percentile(workloads, author)
    author_tenure = get_percentile(tenures, author)

    releases_with_commit = git_repo.git.tag("--contains", commit_hash, "--sort=creatordate", "--format=%(refname:strip=2);%(creatordate:iso)").splitlines()
    if len(releases_with_commit) > 0:
        nearest_release = next(r for r in releases_with_commit if r.split(";")[1] != "")
        nearest_release_name = nearest_release.split(";")[0]
        nearest_release_date = dt.datetime.strptime(nearest_release.split(";")[1], "%Y-%m-%d %H:%M:%S %z")
    else:
        nearest_release_name = ""
        nearest_release_date = ""

    added_lines = 0
    removed_lines = 0
    for mod in commit.modifications:
        added_lines += mod.added
        removed_lines += mod.removed
    
    commits_before_fix = git_repo.git.rev_list("--count", "{}..{}".format(commit_hash, fix_hash))

    commit_info = {
        "hash": commit_hash,
        "message": message,
        "date": str(date),
        "author": author,
        "author_workload": round(author_workload, 3),
        "author_tenure": round(author_tenure, 3),
        "nearest_release_name": str(nearest_release_name),
        "nearest_release_date": str(nearest_release_date),
        "added_lines": added_lines,
        "removed_lines": removed_lines,
        "commits_before_fix": int(commits_before_fix),
        "file": modified_file,
    }
    return commit_info


def get_creating_commit(git_repo, fix_hash, file, commits, authors_first_date):
    # Follow commits, counting renames too
    creating_commit_hash = git_repo.git.log(
        "--follow", "--author-date-order", "--pretty=format:%H", fix_hash, "--", file).splitlines()[-1]
    creating_commit = get_commit_info(git_repo, creating_commit_hash, fix_hash, file, commits, authors_first_date)
    creating_commit["commits_since_file_creation"] = 1
    return creating_commit


def get_inducing_commits(git_repo, fix_commit, fix_files, commits, authors_first_date):
    # Blames is a dict {file: set(commit hashes)}
    blames = git_repo.get_commits_last_modified_lines(fix_commit)
    fix_hash = fix_commit.hash

    blamed_commits = []
    for b_file, b_commits in blames.items():
        if is_invalid_extension(b_file) or is_invalid_file(b_file) or is_test_file(b_file):
            continue
        for inducing_hash in b_commits:
            blamed_info = get_commit_info(git_repo, inducing_hash, fix_hash, b_file, commits, authors_first_date)
            if blamed_info["added_lines"] == 0 and blamed_info["removed_lines"] == 0:
                print("Discarding blamed {}: no actual modifictions".format(inducing_hash))
                # We ignore blamed commits with no actual modifications
                continue
            # Follow commits, counting renames too. We remove possible duplicates
            commit_hashes_b_file = list(dict.fromkeys(git_repo.git.log("--follow", "--author-date-order", "--pretty=format:%H", fix_hash, "--", b_file).splitlines()))[::-1]
            try:
                commits_since_file_creation = commit_hashes_b_file.index(inducing_hash) + 1
            except ValueError:
                # We go there if the blamed inducing commit should be a merge commit, so we repeat the git log again with -m options
                commit_hashes_b_file_merges = list(dict.fromkeys(git_repo.git.log("--follow", "-m", "--author-date-order", "--pretty=format:%H", fix_hash, "--", b_file).splitlines()))[::-1]
                try:
                    commits_since_file_creation = commit_hashes_b_file_merges.index(inducing_hash) + 1
                except ValueError:
                    print("Discarding blamed {}: not found in the log of {}".format(inducing_hash, b_file))
                    continue
            blamed_info["commits_since_file_creation"] = int(commits_since_file_creation)
            blamed_commits.append(blamed_info)

    # If there are some fix_files that were not blamed, then consider the file-creating commits instead
    blamed_files = set([blamed_commit["file"] for blamed_commit in blamed_commits])
    remaining_files = set(fix_files) - blamed_files
    creating_commits = []
    for remaining_file in remaining_files:
        print("No blamed commit for {}: considering its creating commit".format(remaining_file))
        creating_commit = get_creating_commit(git_repo, fix_hash, remaining_file, commits, authors_first_date)
        # If the unblamed file is created in the fixing commit itself, then it is pointless adding its creating commit
        if creating_commit["hash"] != fix_hash:
            creating_commits.append(creating_commit)
    inducing_commits = blamed_commits + creating_commits
    return inducing_commits


def enrich_fixes(git_repo, fixes, commits, authors_first_date):
    enriched_fixes = []
    for fix in fixes:
        fix_hash = fix["hash"]
        # If the commit is not found in the repository, skip it
        try:
            fix_commit = git_repo.get_commit(fix_hash)
        except (ValueError, BadName):
            print("Invalid fixing commit: not found in history")
            continue
        # Do not consider merge fix commits since they have no actual modifications
        if fix_commit.merge:
            print("Invalid fixing commit: it is a merge")
            continue
        # Exclude commits that do not belong to any branch
        if len(fix_commit.branches) == 1 and next(iter(fix_commit.branches)) == '':
            print("Invalid fixing commit: not found in any branch")
            continue

        fix_message = get_adjusted_message(fix_commit.msg)
        fix_date = fix_commit.author_date
        fix_author = fix_commit.author.email
        fix_files = []
        fix_added_lines = 0
        fix_removed_lines = 0
        for mod in fix_commit.modifications:
            file_path = mod.old_path
            # If this happens, then the file has been created in this commit
            if file_path == None:
                file_path = mod.new_path
            if is_invalid_extension(file_path) or is_invalid_file(file_path) or is_test_file(file_path):
                continue
            fix_files.append(file_path)
            fix_added_lines += mod.added
            fix_removed_lines += mod.removed

        if len(fix_files) == 0:
            print("Invalid fixing commit: no relevant files")
            continue        
        if fix_added_lines == 0 and fix_removed_lines == 0:
            print("Invalid fixing commit: no actual modifications")
            continue

        inducings = get_inducing_commits(git_repo, fix_commit, fix_files, commits, authors_first_date)

        # If there are no inducings, we discard it
        if len(inducings) == 0:
            print("Invalid fixing commit: no inducings commits")
            continue

        enriched_fixes.append({
            "hash": fix_hash,
            "message": fix_message,
            "date": str(fix_date),
            "author": fix_author,
            "files": fix_files,
            "added_lines": fix_added_lines,
            "removed_lines": fix_removed_lines,
            "inducings": inducings
        })

    if len(enriched_fixes) > 0:
        last_fix = max(enriched_fixes, key=lambda f: f["date"])
        for enriched_fix in enriched_fixes:
            enriched_fix["commits_before_last_fix"] = int(git_repo.git.rev_list("--count", "{}..{}".format(enriched_fix["hash"], last_fix["hash"])))
    return enriched_fixes


# Import from Json
cves = []
with open(in_filepath, "r") as in_file:
    cves = json.load(in_file)

repos = get_repos(cves)

# Exclude repositories already processes
processed_repos = []
if os.path.exists(out_filepath) and os.path.getsize(out_filepath):
    with open(out_filepath, 'r') as out_file:
        enriched_repos = json.load(out_file)
        processed_repos = [repo["repo"] for repo in enriched_repos]
repos = [repo for repo in repos if repo not in processed_repos]

# Uncomment one of the following lines to exclude or consider only Linux
#repos.remove("https://github.com/torvalds/linux")
# repos = ["https://github.com/torvalds/linux"]

# TODO Remove these lines
#repos.remove("https://github.com/WebKit/webkit")
#repos.remove("https://github.com/liferay/liferay-portal")

num_repos = len(repos)
print("Repositories: {}".format(num_repos))

current_repo = 0
for repo in repos:
    current_repo += 1
    print("\n({}/{}) Working on {}".format(current_repo, num_repos, repo))
    try:
        git_repo = clone_repo(repo)
    except GitCommandError:
        print("Clone failed, skipping repository...")
        continue
    commits = list(git_repo.get_list_commits())
    authors_first_date = get_authors_first_authored_date(commits)
    first_commit_date = commits[0].author_date

    repo_cves = list(filter(lambda x: x["fixes"][0]["repo"] == repo, cves))

    num_repo_cves = len(repo_cves)
    print("{} has {} CVEs".format(repo, num_repo_cves))

    enriched_cves = []
    current_repo_cve = 0
    for repo_cve in repo_cves:
        current_repo_cve += 1
        print("\nProcessing {} ({}/{})".format(repo_cve["cve"], current_repo_cve, num_repo_cves))
        enriched_fixes = enrich_fixes(git_repo, repo_cve["fixes"], commits, authors_first_date)
        if len(enriched_fixes) == 0:
            print("Discarding {}: no viable fixing commits".format(repo_cve["cve"]))
            continue
        enriched_cves.append({
            "cve": repo_cve["cve"],
            "cwe": repo_cve["cwe"],
            "fixes": enriched_fixes
        })
    if len(enriched_cves) == 0:
        print("Discarding {}: no viable CVEs".format(repo))
    else:
        enriched_repo = {
            "repo": repo,
            "creation_date": str(first_commit_date),
            "cves": enriched_cves
        }

        # Export to Json without deleting past data
        if not os.path.exists(out_filepath):
            with open(out_filepath, 'w'):
                pass
        enriched_repos = []
        if os.path.exists(out_filepath) and os.path.getsize(out_filepath):
            with open(out_filepath, 'r') as out_file:
                enriched_repos = json.load(out_file)
        enriched_repos.append(enriched_repo)
        with open(out_filepath, 'w') as out_file:
            json.dump(enriched_repos, out_file, indent=2)

    shutil.rmtree(tmp + repo.rsplit('/', 1)[1])
